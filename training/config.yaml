# LSTM 訓練配置 v1.1
# 基於實際訓練訃整的最优配置

model:
  type: lstm
  input_size: 44         # ✅ 實際特徵數 (不是 8、或一切它)
  hidden_size: 128       # ✅ GPU 4GB 優化版本
  num_layers: 2          # ✅ Bidirectional LSTM 3+ 效果好
  output_size: 1
  dropout: 0.3           # ✅ 正見化強度
  bidirectional: true    # ✅ 雙樣本 LSTM

training:
  epochs: 200
  batch_size: 16         # ✅ GPU 4GB 時的最优值 (32 會 OOM)
  learning_rate: 0.0005  # ✅ 微微向下
  weight_decay: 0.0001   # L2 正則化
  lookback_window: 60    # ✅ 輸入序列長度
  forecast_horizon: 1    # 預測不积渦
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1

optimization:
  optimizer: adamw       # AdamW 改進
  scheduler: cosine      # Cosine annealing learning rate
  warmup_steps: 500
  patience: 25           # Early stopping 對耐心
  min_delta: 1.0e-6      # 最少改進算何 (確保是數字)

data:
  timeframe: '1h'
  limit: 5000            # 載入 ~3-4 個月數據
  normalize: true
  augmentation: false

device: cuda            # GPU 強化
