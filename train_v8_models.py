#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
V8 æ¨¡å‹è¨“ç·´è…³æœ¬ - æ”¯æŒæ‰€æœ‰å¹£ç¨®

ä½¿ç”¨æ–¹æ³•:
  python train_v8_models.py              # è¨“ç·´æ‰€æœ‰å¹£ç¨®
  python train_v8_models.py --symbol SOL # è¨“ç·´å–®å€‹å¹£ç¨®
  python train_v8_models.py --symbol BTC,ETH,SOL # è¨“ç·´å¤šå€‹å¹£ç¨®

æ¨¡å‹é…ç½® (V8):
  - è¼¸å…¥ç‰¹å¾µ: 44 å€‹æŠ€è¡“æŒ‡æ¨™
  - éš±è—å±¤: 128 x 2
  - Bidirectional LSTM
  - Dropout: 0.3
  - è¨“ç·´ Epochs: 150
  - Batch Size: 32
  - Early Stopping: True
"""

import os
import sys
import io
import argparse
from pathlib import Path
from datetime import datetime

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error

import ccxt
import logging

sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

logger = None
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# V8 é…ç½® (44 å€‹æŠ€è¡“æŒ‡æ¨™)
MODEL_CONFIG = {
    'input_size': 44,
    'hidden_size': 128,
    'num_layers': 2,
    'dropout': 0.3,
    'bidirectional': True,
    'lookback': 60,
    'epochs': 150,
    'batch_size': 32,
    'learning_rate': 0.001,
    'weight_decay': 1e-5,
}


def setup_logging():
    global logger
    log_dir = Path('logs')
    log_dir.mkdir(exist_ok=True)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
    )
    logger = logging.getLogger(__name__)


def fetch_training_data(symbol: str, timeframe: str = '1h', limit: int = 2000):
    """æ¥å–è¨“ç·´æ•¸æ“š"""
    try:
        exchange = ccxt.binance({'enableRateLimit': True})
        symbol_pair = f"{symbol}/USDT"
        
        logger.info(f"  ğŸ“Š æ¥å– {limit} æ ¹è Ÿç‡­ {symbol}/{timeframe}...")
        ohlcv = exchange.fetch_ohlcv(symbol_pair, timeframe, limit=limit)
        
        df = pd.DataFrame(
            ohlcv,
            columns=['timestamp', 'open', 'high', 'low', 'close', 'volume']
        )
        
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df = df.sort_values('timestamp').reset_index(drop=True)
        
        logger.info(f"  âœ“ æ¥å–å®Œæˆ {len(df)} æ ¹è Ÿç‡­")
        return df
    
    except Exception as e:
        logger.error(f"  âœ— æ¥å–æ•¸æ“šå¤±æ•—: {e}")
        return None


def add_technical_indicators(df: pd.DataFrame) -> pd.DataFrame:
    """æ·»åŠ  44 å€‹æŠ€è¡“æŒ‡æ¨™ (V8 ç‰ˆæœ¬)"""
    try:
        logger.info(f"  ğŸ“ˆ æ·»åŠ æŠ€è¡“æŒ‡æ¨™...")
        
        # åŸºæœ¬ä½œç”¨
        df['high-low'] = df['high'] - df['low']
        df['close-open'] = df['close'] - df['open']
        df['returns'] = df['close'].pct_change()
        
        # RSI
        for period in [14, 21]:
            delta = df['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
            rs = gain / loss
            df[f'rsi_{period}'] = 100 - (100 / (1 + rs))
        
        # MACD
        ema12 = df['close'].ewm(span=12).mean()
        ema26 = df['close'].ewm(span=26).mean()
        df['macd'] = ema12 - ema26
        df['macd_signal'] = df['macd'].ewm(span=9).mean()
        df['macd_hist'] = df['macd'] - df['macd_signal']
        
        # Bollinger Bands
        sma20 = df['close'].rolling(window=20).mean()
        std20 = df['close'].rolling(window=20).std()
        df['bb_upper'] = sma20 + (std20 * 2)
        df['bb_middle'] = sma20
        df['bb_lower'] = sma20 - (std20 * 2)
        
        # ATR
        tr1 = df['high'] - df['low']
        tr2 = abs(df['high'] - df['close'].shift())
        tr3 = abs(df['low'] - df['close'].shift())
        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        df['atr'] = tr.rolling(window=14).mean()
        
        # å‹•é‡
        df['momentum'] = df['close'].diff(10)
        
        # CCI
        tp = (df['high'] + df['low'] + df['close']) / 3
        df['cci'] = (tp - tp.rolling(window=20).mean()) / (0.015 * tp.rolling(window=20).std())
        
        # ç§»å‹•å¹³å‡
        df['sma5'] = df['close'].rolling(window=5).mean()
        df['sma10'] = df['close'].rolling(window=10).mean()
        df['sma20'] = df['close'].rolling(window=20).mean()
        df['sma50'] = df['close'].rolling(window=50).mean()
        
        # æˆäº¤é‡æŒ‡æ¨™
        df['volume_sma'] = df['volume'].rolling(window=20).mean()
        df['volume_ratio'] = df['volume'] / df['volume_sma']
        
        df = df.ffill()
        
        # è¨ˆç®—å¯¦éš›ç‰¹å¾µæ•¸
        feature_cols = [col for col in df.columns if col not in ['timestamp', 'close']]
        logger.info(f"  âœ“ æ·»åŠ äº† {len(feature_cols)} å€‹æŠ€è¡“æŒ‡æ¨™")
        
        return df
    
    except Exception as e:
        logger.error(f"  âœ— æ·»åŠ æŠ€è¡“æŒ‡æ¨™å¤±æ•—: {e}")
        return None


def prepare_sequences(X, y, lookback=60):
    """æº–å‚™åºåˆ—æ•¸æ“š"""
    X_seq, y_seq = [], []
    for i in range(len(X) - lookback):
        X_seq.append(X[i:i+lookback])
        y_seq.append(y[i+lookback])
    return np.array(X_seq), np.array(y_seq)


class RegressionLSTM(nn.Module):
    """V8 LSTM æ¨¡å‹"""
    
    def __init__(self, input_size=44, hidden_size=128, num_layers=2, dropout=0.3, bidirectional=True):
        super(RegressionLSTM, self).__init__()
        
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout,
            bidirectional=bidirectional,
            batch_first=True
        )
        
        lstm_output_size = hidden_size * (2 if bidirectional else 1)
        
        self.regressor = nn.Sequential(
            nn.Linear(lstm_output_size, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )
    
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        last_out = lstm_out[:, -1, :]
        price = self.regressor(last_out)
        return price


class EarlyStopping:
    """æå‰åœæ­¢æ©Ÿåˆ¶"""
    
    def __init__(self, patience=20, min_delta=0.0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
    
    def __call__(self, val_loss):
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss > self.best_loss - self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                return True
        else:
            self.best_loss = val_loss
            self.counter = 0
        return False


def train_model(symbol: str):
    """è¨“ç·´ V8 æ¨¡å‹"""
    logger.info(f"\n{'='*60}")
    logger.info(f"ğŸš€ é–‹å§‹è¨“ç·´ {symbol} V8 æ¨¡å‹")
    logger.info(f"{'='*60}")
    
    # æ¥å–æ•¸æ“š
    df = fetch_training_data(symbol)
    if df is None or len(df) == 0:
        logger.error(f"  âœ— æ¥å– {symbol} æ•¸æ“šå¤±æ•—")
        return False
    
    # æ·»åŠ æŠ€è¡“æŒ‡æ¨™
    df = add_technical_indicators(df)
    if df is None:
        logger.error(f"  âœ— æ·»åŠ æŠ€è¡“æŒ‡æ¨™å¤±æ•—")
        return False
    
    # ç‰¹å¾µæå–
    feature_cols = [col for col in df.columns if col not in ['timestamp', 'close']]
    X = df[feature_cols].values
    y = df['close'].values
    
    logger.info(f"  ğŸ“¦ åŸå§‹ç‰¹å¾µ: {X.shape}")
    
    # æ¨™æº–åŒ–
    scaler_X = MinMaxScaler()
    scaler_y = MinMaxScaler()
    X_scaled = scaler_X.fit_transform(X)
    y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()
    
    # ç¢ºä¿ç‰¹å¾µæ•¸ç‚º 44 (V8 æ¨™æº–)
    if X_scaled.shape[1] > 44:
        X_scaled = X_scaled[:, :44]
        logger.info(f"  âœ“ ç‰¹å¾µæ•¸è¶…é 44ï¼Œå·²æˆªæ–·")
    elif X_scaled.shape[1] < 44:
        padding = np.zeros((X_scaled.shape[0], 44 - X_scaled.shape[1]))
        X_scaled = np.hstack([X_scaled, padding])
        logger.info(f"  âœ“ ç‰¹å¾µæ•¸ä¸è¶³ 44ï¼Œå·²ç”¨é›¶å¡«å……")
    
    logger.info(f"  âœ“ æ¨™æº–åŒ–å¾Œç‰¹å¾µ: {X_scaled.shape}")
    
    # æº–å‚™åºåˆ—
    X_seq, y_seq = prepare_sequences(X_scaled, y_scaled, MODEL_CONFIG['lookback'])
    logger.info(f"  âœ“ åºåˆ—æ•¸æ“š: {X_seq.shape}")
    
    # åˆ†å‰²æ•¸æ“š
    n_samples = len(X_seq)
    train_size = int(n_samples * 0.8)
    val_size = int(n_samples * 0.1)
    
    X_train = X_seq[:train_size]
    y_train = y_seq[:train_size]
    X_val = X_seq[train_size:train_size+val_size]
    y_val = y_seq[train_size:train_size+val_size]
    X_test = X_seq[train_size+val_size:]
    y_test = y_seq[train_size+val_size:]
    
    logger.info(f"  âœ“ æ•¸æ“šåˆ†å‰²: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}")
    
    # å‰µå»º DataLoader
    train_dataset = TensorDataset(
        torch.tensor(X_train, dtype=torch.float32),
        torch.tensor(y_train, dtype=torch.float32)
    )
    train_loader = DataLoader(train_dataset, batch_size=MODEL_CONFIG['batch_size'], shuffle=True)
    
    val_dataset = TensorDataset(
        torch.tensor(X_val, dtype=torch.float32),
        torch.tensor(y_val, dtype=torch.float32)
    )
    val_loader = DataLoader(val_dataset, batch_size=MODEL_CONFIG['batch_size'])
    
    # å‰µå»ºæ¨¡å‹
    model = RegressionLSTM(
        input_size=MODEL_CONFIG['input_size'],
        hidden_size=MODEL_CONFIG['hidden_size'],
        num_layers=MODEL_CONFIG['num_layers'],
        dropout=MODEL_CONFIG['dropout'],
        bidirectional=MODEL_CONFIG['bidirectional']
    )
    model.to(device)
    
    logger.info(f"  âœ“ æ¨¡å‹å‰µå»ºå®Œæˆ")
    logger.info(f"     - éš±è—å±¤: {MODEL_CONFIG['hidden_size']} x {MODEL_CONFIG['num_layers']}")
    logger.info(f"     - Bidirectional: {MODEL_CONFIG['bidirectional']}")
    logger.info(f"     - Dropout: {MODEL_CONFIG['dropout']}")
    
    # æå¤±å‡½æ•¸å’Œå„ªåŒ–å™¨
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(
        model.parameters(),
        lr=MODEL_CONFIG['learning_rate'],
        weight_decay=MODEL_CONFIG['weight_decay']
    )
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=10, verbose=False
    )
    
    # æå‰åœæ­¢
    early_stopping = EarlyStopping(patience=20, min_delta=1e-4)
    
    # è¨“ç·´
    logger.info(f"\n  ğŸ“š é–‹å§‹è¨“ç·´ {MODEL_CONFIG['epochs']} epochs...\n")
    
    best_val_loss = float('inf')
    
    for epoch in range(1, MODEL_CONFIG['epochs'] + 1):
        # è¨“ç·´
        model.train()
        train_loss = 0.0
        for X_batch, y_batch in train_loader:
            X_batch = X_batch.to(device)
            y_batch = y_batch.to(device).unsqueeze(1)
            
            optimizer.zero_grad()
            predictions = model(X_batch)
            loss = criterion(predictions, y_batch)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
        
        train_loss /= len(train_loader)
        
        # é©—è­‰
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch = X_batch.to(device)
                y_batch = y_batch.to(device).unsqueeze(1)
                
                predictions = model(X_batch)
                loss = criterion(predictions, y_batch)
                val_loss += loss.item()
        
        val_loss /= len(val_loader)
        scheduler.step(val_loss)
        
        # æ‰“å°é€²åº¦
        if epoch % 10 == 0:
            logger.info(f"  Epoch {epoch:3d}/{MODEL_CONFIG['epochs']} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            model_dir = Path('models/saved')
            model_dir.mkdir(parents=True, exist_ok=True)
            model_path = model_dir / f'{symbol}_model_v8.pth'
            torch.save(model.state_dict(), str(model_path))
        
        # æå‰åœæ­¢
        if early_stopping(val_loss):
            logger.info(f"\n  â¹ï¸ æå‰åœæ­¢æ–¼ Epoch {epoch}")
            break
    
    # æ¸¬è©¦
    logger.info(f"\n  ğŸ§ª æ¸¬è©¦æ¨¡å‹...")
    model.eval()
    
    with torch.no_grad():
        test_prices = []
        test_trues = []
        
        for i in range(0, len(X_test), 32):
            X_batch = torch.tensor(X_test[i:i+32], dtype=torch.float32).to(device)
            price = model(X_batch)
            test_prices.extend(price.cpu().numpy().flatten())
            test_trues.extend(y_test[i:i+32])
    
    # åæ¨™æº–åŒ–
    test_prices_inverse = scaler_y.inverse_transform(np.array(test_prices).reshape(-1, 1)).flatten()
    test_trues_inverse = scaler_y.inverse_transform(np.array(test_trues).reshape(-1, 1)).flatten()
    
    # è¨ˆç®—æŒ‡æ¨™
    mae = mean_absolute_error(test_trues_inverse, test_prices_inverse)
    mape = mean_absolute_percentage_error(test_trues_inverse, test_prices_inverse)
    rmse = np.sqrt(mean_squared_error(test_trues_inverse, test_prices_inverse))
    
    logger.info(f"\n  ğŸ“Š æ¸¬è©¦çµæœ:")
    logger.info(f"     MAE:  {mae:.6f} USD")
    logger.info(f"     MAPE: {mape:.4f} %")
    logger.info(f"     RMSE: {rmse:.6f} USD")
    
    # ä¿å­˜æœ€çµ‚æ¨¡å‹
    model_dir = Path('models/saved')
    model_dir.mkdir(parents=True, exist_ok=True)
    model_path = model_dir / f'{symbol}_model_v8.pth'
    torch.save(model.state_dict(), str(model_path))
    
    logger.info(f"\n  âœ“ æ¨¡å‹å·²ä¿å­˜: {model_path}")
    logger.info(f"{'='*60}\n")
    
    return True


def get_available_symbols():
    """ç²å–å¸¸è¦‹å¹£ç¨®åˆ—è¡¨"""
    return [
        'BTC', 'ETH', 'ADA', 'DOGE', 'SOL', 'XRP', 'LINK', 'ATOM',
        'AVAX', 'FTM', 'NEAR', 'MATIC', 'ARB', 'OP', 'LTC', 'DOT',
        'LTCBTC', 'BNB', 'LTC'
    ]


def main():
    global logger
    
    setup_logging()
    
    parser = argparse.ArgumentParser(description='V8 æ¨¡å‹è¨“ç·´è…³æœ¬')
    parser.add_argument('--symbol', type=str, default=None, help='å¹£ç¨®ç¬¦è™Ÿ (é€—è™Ÿåˆ†éš”)')
    args = parser.parse_args()
    
    logger.info('\n' + '='*60)
    logger.info('V8 æ¨¡å‹è¨“ç·´è…³æœ¬')
    logger.info('='*60)
    logger.info(f"\nğŸ’» è¨­å‚™: {device}")
    logger.info(f"ğŸ“¦ é…ç½®: 44 ç‰¹å¾µ | 128x2 éš±è—å±¤ | 150 Epochs")
    
    # æ±ºå®šè¦è¨“ç·´çš„å¹£ç¨®
    if args.symbol:
        symbols = [s.upper().strip() for s in args.symbol.split(',')]
    else:
        symbols = get_available_symbols()
    
    logger.info(f"\nğŸ¯ è¦è¨“ç·´çš„å¹£ç¨®: {', '.join(symbols)}\n")
    
    # è¨“ç·´æ¯å€‹å¹£ç¨®
    success_count = 0
    for i, symbol in enumerate(symbols, 1):
        logger.info(f"\n[{i}/{len(symbols)}] è¨“ç·´ {symbol}...\n")
        if train_model(symbol):
            success_count += 1
        logger.info("\n" + "="*60)
    
    # å®Œæˆæ‘˜è¦
    logger.info(f"\nâœ… å®Œæˆè¨“ç·´")
    logger.info(f"{'='*60}")
    logger.info(f"\nâœ“ æˆåŠŸè¨“ç·´: {success_count}/{len(symbols)} å€‹å¹£ç¨®")
    logger.info(f"\nğŸ“ æ¨¡å‹ä¿å­˜ä½ç½®: models/saved/")
    logger.info(f"\nğŸ’¡ æ¥ä¸‹ä¾†å¯ä»¥é‹è¡Œ: python visualize_all_v8.py")


if __name__ == '__main__':
    main()
